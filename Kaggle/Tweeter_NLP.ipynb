{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmA6EzkQJ5jt"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "embedding_dim = 200\n",
    "max_length = 16\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size=160000\n",
    "test_portion=.1\n",
    "\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7135,
     "status": "ok",
     "timestamp": 1559930021999,
     "user": {
      "displayName": "Laurence Moroney",
      "photoUrl": "https://lh4.googleusercontent.com/-wUzpekukCVw/AAAAAAAAAAI/AAAAAAAAAHw/pQPstOOJqqE/s64/photo.jpg",
      "userId": "17858265307580721507"
     },
     "user_tz": 420
    },
    "id": "bM0l_dORKqE0",
    "outputId": "491ba86b-f780-4355-a4be-765565a29c8c"
   },
   "outputs": [],
   "source": [
    "# Note that I cleaned the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader\n",
    "# You can do that yourself with:\n",
    "# iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv\n",
    "# I then hosted it on my site to make it easier to use in this notebook\n",
    "\n",
    "#!wget --no-check-certificate \\\n",
    "#    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n",
    "#    -O /tmp/training_cleaned.csv\n",
    "\n",
    "# Your Code here. Create list items where the first item is the text, found in row[5], \n",
    "# and the second is the label. Note that the label is a '0' or a '4' in the text. When it's the former, make\n",
    "# your label to be 0, otherwise 1. Keep a count of the number of sentences in num_sentences\n",
    "\n",
    "num_sentences = 0\n",
    "\n",
    "with open(\"./datasets/sentiment140/training_cleaned.csv\", encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        list_item=[]\n",
    "        list_item.append(row[5])\n",
    "        this_label=row[0]\n",
    "        if this_label=='0':\n",
    "            list_item.append(0)\n",
    "        else:\n",
    "            list_item.append(1)\n",
    "        num_sentences = num_sentences + 1\n",
    "        corpus.append(list_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1559930024089,
     "user": {
      "displayName": "Laurence Moroney",
      "photoUrl": "https://lh4.googleusercontent.com/-wUzpekukCVw/AAAAAAAAAAI/AAAAAAAAAHw/pQPstOOJqqE/s64/photo.jpg",
      "userId": "17858265307580721507"
     },
     "user_tz": 420
    },
    "id": "3kxblBUjEUX-",
    "outputId": "3c0227a2-e74b-4d9b-cabb-f9ee150571b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "1600000\n",
      "[\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]\n"
     ]
    }
   ],
   "source": [
    "print(num_sentences)\n",
    "print(len(corpus))\n",
    "print(corpus[1])\n",
    "\n",
    "# Expected Output:\n",
    "# 1600000\n",
    "# 1600000\n",
    "# [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13089,
     "status": "ok",
     "timestamp": 1559930143373,
     "user": {
      "displayName": "Laurence Moroney",
      "photoUrl": "https://lh4.googleusercontent.com/-wUzpekukCVw/AAAAAAAAAAI/AAAAAAAAAHw/pQPstOOJqqE/s64/photo.jpg",
      "userId": "17858265307580721507"
     },
     "user_tz": 420
    },
    "id": "C1zdgJkusRh0",
    "outputId": "b6edd322-8191-45e7-cb12-08921685a72f"
   },
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sequence(text_sequence):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohOGz24lsNAD"
   },
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "labels=[]\n",
    "random.shuffle(corpus)\n",
    "for x in range(training_size):\n",
    "    sentences.append(corpus[x][0])\n",
    "    labels.append(corpus[x][1])\n",
    "\n",
    "sequences = [tokenize_sequence(sequence) for sequence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in sequences:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(test_portion * training_size)\n",
    "\n",
    "test_sequences = padded[0:split]\n",
    "training_sequences = padded[split:training_size]\n",
    "test_labels = labels[0:split]\n",
    "training_labels = labels[split:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKKvbuEBOGFz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 200)         6104400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 64)          64064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 6,201,553\n",
      "Trainable params: 97,153\n",
      "Non-trainable params: 6,104,400\n",
      "_________________________________________________________________\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/50\n",
      "144000/144000 - 22s - loss: 0.6205 - accuracy: 0.6315 - val_loss: 0.5363 - val_accuracy: 0.7350\n",
      "Epoch 2/50\n",
      "144000/144000 - 21s - loss: 0.5116 - accuracy: 0.7482 - val_loss: 0.4982 - val_accuracy: 0.7563\n",
      "Epoch 3/50\n",
      "144000/144000 - 21s - loss: 0.4765 - accuracy: 0.7726 - val_loss: 0.4917 - val_accuracy: 0.7627\n",
      "Epoch 4/50\n",
      "144000/144000 - 21s - loss: 0.4556 - accuracy: 0.7860 - val_loss: 0.4825 - val_accuracy: 0.7645\n",
      "Epoch 5/50\n",
      "144000/144000 - 21s - loss: 0.4400 - accuracy: 0.7947 - val_loss: 0.4810 - val_accuracy: 0.7719\n",
      "Epoch 6/50\n",
      "144000/144000 - 20s - loss: 0.4280 - accuracy: 0.8004 - val_loss: 0.4885 - val_accuracy: 0.7707\n",
      "Epoch 7/50\n",
      "144000/144000 - 21s - loss: 0.4191 - accuracy: 0.8064 - val_loss: 0.5065 - val_accuracy: 0.7617\n",
      "Epoch 8/50\n",
      "144000/144000 - 21s - loss: 0.4111 - accuracy: 0.8106 - val_loss: 0.4776 - val_accuracy: 0.7738\n",
      "Epoch 9/50\n",
      "144000/144000 - 20s - loss: 0.4029 - accuracy: 0.8150 - val_loss: 0.5014 - val_accuracy: 0.7704\n",
      "Epoch 10/50\n",
      "144000/144000 - 20s - loss: 0.3983 - accuracy: 0.8181 - val_loss: 0.4835 - val_accuracy: 0.7751\n",
      "Epoch 11/50\n",
      "144000/144000 - 21s - loss: 0.3920 - accuracy: 0.8218 - val_loss: 0.4849 - val_accuracy: 0.7744\n",
      "Epoch 12/50\n",
      "144000/144000 - 21s - loss: 0.3858 - accuracy: 0.8242 - val_loss: 0.4863 - val_accuracy: 0.7721\n",
      "Epoch 13/50\n",
      "144000/144000 - 21s - loss: 0.3813 - accuracy: 0.8283 - val_loss: 0.4714 - val_accuracy: 0.7803\n",
      "Epoch 14/50\n",
      "144000/144000 - 21s - loss: 0.3782 - accuracy: 0.8294 - val_loss: 0.4720 - val_accuracy: 0.7830\n",
      "Epoch 15/50\n",
      "144000/144000 - 21s - loss: 0.3735 - accuracy: 0.8311 - val_loss: 0.4769 - val_accuracy: 0.7784\n",
      "Epoch 16/50\n",
      "144000/144000 - 21s - loss: 0.3701 - accuracy: 0.8337 - val_loss: 0.5104 - val_accuracy: 0.7794\n",
      "Epoch 17/50\n",
      "144000/144000 - 21s - loss: 0.3652 - accuracy: 0.8369 - val_loss: 0.5088 - val_accuracy: 0.7761\n",
      "Epoch 18/50\n",
      "144000/144000 - 22s - loss: 0.3634 - accuracy: 0.8366 - val_loss: 0.4863 - val_accuracy: 0.7821\n",
      "Epoch 19/50\n",
      "144000/144000 - 21s - loss: 0.3599 - accuracy: 0.8389 - val_loss: 0.4913 - val_accuracy: 0.7797\n",
      "Epoch 20/50\n",
      "144000/144000 - 21s - loss: 0.3566 - accuracy: 0.8402 - val_loss: 0.5019 - val_accuracy: 0.7779\n",
      "Epoch 21/50\n",
      "144000/144000 - 20s - loss: 0.3549 - accuracy: 0.8408 - val_loss: 0.4863 - val_accuracy: 0.7777\n",
      "Epoch 22/50\n",
      "144000/144000 - 21s - loss: 0.3510 - accuracy: 0.8444 - val_loss: 0.5007 - val_accuracy: 0.7746\n",
      "Epoch 23/50\n",
      "144000/144000 - 21s - loss: 0.3477 - accuracy: 0.8448 - val_loss: 0.4876 - val_accuracy: 0.7803\n",
      "Epoch 24/50\n",
      "144000/144000 - 21s - loss: 0.3468 - accuracy: 0.8454 - val_loss: 0.5439 - val_accuracy: 0.7803\n",
      "Epoch 25/50\n",
      "144000/144000 - 21s - loss: 0.3452 - accuracy: 0.8452 - val_loss: 0.4999 - val_accuracy: 0.7768\n",
      "Epoch 26/50\n",
      "144000/144000 - 21s - loss: 0.3439 - accuracy: 0.8469 - val_loss: 0.5095 - val_accuracy: 0.7794\n",
      "Epoch 27/50\n",
      "144000/144000 - 21s - loss: 0.3412 - accuracy: 0.8475 - val_loss: 0.4998 - val_accuracy: 0.7794\n",
      "Epoch 28/50\n",
      "144000/144000 - 21s - loss: 0.3374 - accuracy: 0.8509 - val_loss: 0.5163 - val_accuracy: 0.7755\n",
      "Epoch 29/50\n",
      "144000/144000 - 21s - loss: 0.3367 - accuracy: 0.8501 - val_loss: 0.5087 - val_accuracy: 0.7786\n",
      "Epoch 30/50\n",
      "144000/144000 - 21s - loss: 0.3332 - accuracy: 0.8518 - val_loss: 0.4954 - val_accuracy: 0.7766\n",
      "Epoch 31/50\n",
      "144000/144000 - 20s - loss: 0.3323 - accuracy: 0.8528 - val_loss: 0.5208 - val_accuracy: 0.7733\n",
      "Epoch 32/50\n",
      "144000/144000 - 20s - loss: 0.3306 - accuracy: 0.8536 - val_loss: 0.5193 - val_accuracy: 0.7770\n",
      "Epoch 33/50\n",
      "144000/144000 - 20s - loss: 0.3303 - accuracy: 0.8531 - val_loss: 0.5147 - val_accuracy: 0.7744\n",
      "Epoch 34/50\n",
      "144000/144000 - 20s - loss: 0.3281 - accuracy: 0.8544 - val_loss: 0.5201 - val_accuracy: 0.7763\n",
      "Epoch 35/50\n",
      "144000/144000 - 20s - loss: 0.3243 - accuracy: 0.8572 - val_loss: 0.5221 - val_accuracy: 0.7748\n",
      "Epoch 36/50\n",
      "144000/144000 - 20s - loss: 0.3231 - accuracy: 0.8576 - val_loss: 0.4987 - val_accuracy: 0.7788\n",
      "Epoch 37/50\n",
      "144000/144000 - 20s - loss: 0.3235 - accuracy: 0.8585 - val_loss: 0.4946 - val_accuracy: 0.7780\n",
      "Epoch 38/50\n",
      "144000/144000 - 20s - loss: 0.3212 - accuracy: 0.8578 - val_loss: 0.5184 - val_accuracy: 0.7804\n",
      "Epoch 39/50\n",
      "144000/144000 - 20s - loss: 0.3177 - accuracy: 0.8594 - val_loss: 0.5216 - val_accuracy: 0.7782\n",
      "Epoch 40/50\n",
      "144000/144000 - 20s - loss: 0.3168 - accuracy: 0.8597 - val_loss: 0.5173 - val_accuracy: 0.7759\n",
      "Epoch 41/50\n",
      "144000/144000 - 20s - loss: 0.3153 - accuracy: 0.8614 - val_loss: 0.5050 - val_accuracy: 0.7757\n",
      "Epoch 42/50\n",
      "144000/144000 - 20s - loss: 0.3144 - accuracy: 0.8619 - val_loss: 0.5350 - val_accuracy: 0.7765\n",
      "Epoch 43/50\n",
      "144000/144000 - 20s - loss: 0.3138 - accuracy: 0.8626 - val_loss: 0.5255 - val_accuracy: 0.7741\n",
      "Epoch 44/50\n",
      "144000/144000 - 20s - loss: 0.3099 - accuracy: 0.8636 - val_loss: 0.5240 - val_accuracy: 0.7756\n",
      "Epoch 45/50\n",
      "144000/144000 - 20s - loss: 0.3112 - accuracy: 0.8630 - val_loss: 0.5155 - val_accuracy: 0.7765\n",
      "Epoch 46/50\n",
      "144000/144000 - 20s - loss: 0.3105 - accuracy: 0.8640 - val_loss: 0.5301 - val_accuracy: 0.7762\n",
      "Epoch 47/50\n",
      "144000/144000 - 21s - loss: 0.3104 - accuracy: 0.8644 - val_loss: 0.5370 - val_accuracy: 0.7774\n",
      "Epoch 48/50\n",
      "144000/144000 - 20s - loss: 0.3070 - accuracy: 0.8651 - val_loss: 0.5195 - val_accuracy: 0.7779\n",
      "Epoch 49/50\n",
      "144000/144000 - 21s - loss: 0.3075 - accuracy: 0.8650 - val_loss: 0.5089 - val_accuracy: 0.7791\n",
      "Epoch 50/50\n",
      "144000/144000 - 21s - loss: 0.3051 - accuracy: 0.8664 - val_loss: 0.5274 - val_accuracy: 0.7756\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, trainable=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "num_epochs = 50\n",
    "history = model.fit(x=np.array(training_sequences), y=np.array(training_labels), epochs=num_epochs, validation_data=(np.array(test_sequences),np.array(test_labels)), verbose=2)\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxju4ItJKO8F"
   },
   "outputs": [],
   "source": [
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r')\n",
    "plt.plot(epochs, val_acc, 'b')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r')\n",
    "plt.plot(epochs, val_loss, 'b')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "# Expected Output\n",
    "# A chart where the validation loss does not increase sharply!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of NLP Course - Week 3 Exercise Question.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP%20Course%20-%20Week%203%20Exercise%20Question.ipynb",
     "timestamp": 1581533329242
    },
    {
     "file_id": "1bEdiniaTOd2jRxqGDoluQKXTRmYGJUy0",
     "timestamp": 1559930374235
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

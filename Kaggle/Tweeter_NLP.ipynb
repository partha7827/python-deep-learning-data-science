{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmA6EzkQJ5jt"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "embedding_dim = 200\n",
    "max_length = 16\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size=160000\n",
    "test_portion= .1\n",
    "\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7135,
     "status": "ok",
     "timestamp": 1559930021999,
     "user": {
      "displayName": "Laurence Moroney",
      "photoUrl": "https://lh4.googleusercontent.com/-wUzpekukCVw/AAAAAAAAAAI/AAAAAAAAAHw/pQPstOOJqqE/s64/photo.jpg",
      "userId": "17858265307580721507"
     },
     "user_tz": 420
    },
    "id": "bM0l_dORKqE0",
    "outputId": "491ba86b-f780-4355-a4be-765565a29c8c"
   },
   "outputs": [],
   "source": [
    "# Note that I cleaned the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader\n",
    "# You can do that yourself with:\n",
    "# iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv\n",
    "# I then hosted it on my site to make it easier to use in this notebook\n",
    "\n",
    "#!wget --no-check-certificate \\\n",
    "#    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n",
    "#    -O /tmp/training_cleaned.csv\n",
    "\n",
    "# Your Code here. Create list items where the first item is the text, found in row[5], \n",
    "# and the second is the label. Note that the label is a '0' or a '4' in the text. When it's the former, make\n",
    "# your label to be 0, otherwise 1. Keep a count of the number of sentences in num_sentences\n",
    "\n",
    "num_sentences = 0\n",
    "\n",
    "with open(\"./datasets/sentiment140/training_cleaned.csv\", encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        list_item=[]\n",
    "        list_item.append(row[5])\n",
    "        this_label=row[0]\n",
    "        if this_label=='0':\n",
    "            list_item.append(0)\n",
    "        else:\n",
    "            list_item.append(1)\n",
    "        num_sentences = num_sentences + 1\n",
    "        corpus.append(list_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1559930024089,
     "user": {
      "displayName": "Laurence Moroney",
      "photoUrl": "https://lh4.googleusercontent.com/-wUzpekukCVw/AAAAAAAAAAI/AAAAAAAAAHw/pQPstOOJqqE/s64/photo.jpg",
      "userId": "17858265307580721507"
     },
     "user_tz": 420
    },
    "id": "3kxblBUjEUX-",
    "outputId": "3c0227a2-e74b-4d9b-cabb-f9ee150571b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "1600000\n",
      "[\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]\n"
     ]
    }
   ],
   "source": [
    "print(num_sentences)\n",
    "print(len(corpus))\n",
    "print(corpus[1])\n",
    "\n",
    "# Expected Output:\n",
    "# 1600000\n",
    "# 1600000\n",
    "# [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13089,
     "status": "ok",
     "timestamp": 1559930143373,
     "user": {
      "displayName": "Laurence Moroney",
      "photoUrl": "https://lh4.googleusercontent.com/-wUzpekukCVw/AAAAAAAAAAI/AAAAAAAAAHw/pQPstOOJqqE/s64/photo.jpg",
      "userId": "17858265307580721507"
     },
     "user_tz": 420
    },
    "id": "C1zdgJkusRh0",
    "outputId": "b6edd322-8191-45e7-cb12-08921685a72f"
   },
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sequence(text_sequence):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohOGz24lsNAD"
   },
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "labels=[]\n",
    "random.shuffle(corpus)\n",
    "for x in range(training_size):\n",
    "    sentences.append(corpus[x][0])\n",
    "    labels.append(corpus[x][1])\n",
    "\n",
    "sequences = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sequence, flags=re.MULTILINE) for sequence in sentences]\n",
    "sequences = [tokenize_sequence(sequence) for sequence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in sequences:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(test_portion * training_size)\n",
    "\n",
    "test_sequences = padded[0:split]\n",
    "training_sequences = padded[split:training_size]\n",
    "test_labels = labels[0:split]\n",
    "training_labels = labels[split:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKKvbuEBOGFz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = tf.keras.Sequential([\\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, trainable=False),\\n    tf.keras.layers.Conv1D(256, 5, activation=\\'relu\\'),\\n    tf.keras.layers.Dropout(0.5),\\n    tf.keras.layers.Conv1D(64, 5, activation=\\'relu\\'),\\n    tf.keras.layers.Dropout(0.5),\\n    tf.keras.layers.MaxPooling1D(pool_size=4),\\n    tf.keras.layers.LSTM(64),\\n    tf.keras.layers.Dense(1, activation=\\'sigmoid\\')\\n])\\n\\n\\nmodel.compile(loss=\\'binary_crossentropy\\',optimizer=\\'adam\\',metrics=[\\'accuracy\\'])\\nmodel.summary()\\n\\nnum_epochs = 50\\nhistory = model.fit(x=np.array(training_sequences), y=np.array(training_labels), epochs=num_epochs, validation_data=(np.array(test_sequences),np.array(test_labels)), batch_size = 1024, verbose=2)\\n\\nprint(\"Training Complete\")\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, trainable=False),\n",
    "    tf.keras.layers.Conv1D(256, 5, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "num_epochs = 50\n",
    "history = model.fit(x=np.array(training_sequences), y=np.array(training_labels), epochs=num_epochs, validation_data=(np.array(test_sequences),np.array(test_labels)), batch_size = 1024, verbose=2)\n",
    "\n",
    "print(\"Training Complete\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = tf.keras.Sequential([\\n    tf.keras.layers.Embedding(vocab_size, embedding_dim),\\n    tf.keras.layers.Dropout(0.5),\\n    tf.keras.layers.Conv1D(256, 5),\\n    tf.keras.layers.Dropout(0.5),\\n    tf.keras.layers.Conv1D(128, 5),\\n    tf.keras.layers.MaxPooling1D(pool_size=4),\\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)),\\n    tf.keras.layers.LSTM(128, dropout=0.5, recurrent_dropout=0.5),\\n    tf.keras.layers.Dense(128, activation=\\'relu\\'),\\n    tf.keras.layers.Dense(1, activation=\\'sigmoid\\')\\n])\\n\\n\\nadam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\\nmodel.compile(loss=\\'binary_crossentropy\\',optimizer=adam,metrics=[\\'accuracy\\'])\\nmodel.summary()\\n\\nnum_epochs = 20\\nhistory = model.fit(x=np.array(training_sequences), y=np.array(training_labels), epochs=num_epochs, validation_data=(np.array(test_sequences),np.array(test_labels)),batch_size = 2048, verbose=2)\\n\\nprint(\"Training Complete\")\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(256, 5),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(128, 5),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)),\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "num_epochs = 20\n",
    "history = model.fit(x=np.array(training_sequences), y=np.array(training_labels), epochs=num_epochs, validation_data=(np.array(test_sequences),np.array(test_labels)),batch_size = 2048, verbose=2)\n",
    "\n",
    "print(\"Training Complete\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           (None, 214)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_34 (Embedding)        (None, 214, 200)     6104400     input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, 210, 256)     256256      embedding_34[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 210, 256)     0           conv1d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_93 (Conv1D)              (None, 206, 128)     163968      dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_56 (MaxPooling1D) (None, 51, 128)      0           conv1d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, 210, 256)     256256      embedding_34[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_30 (Bidirectional (None, 51, 512)      788480      max_pooling1d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling1D) (None, 52, 256)      0           conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_81 (LSTM)                  (None, 128)          328192      bidirectional_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_82 (LSTM)                  (None, 128)          197120      max_pooling1d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_91 (Dense)                (None, 128)          16512       lstm_81[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 128)          16512       lstm_82[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 256)          0           dense_91[0][0]                   \n",
      "                                                                 dense_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 1)            257         concatenate_28[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 8,127,953\n",
      "Trainable params: 8,127,953\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/20\n",
      " - 638s - loss: 0.6940 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.4976\n",
      "Epoch 2/20\n",
      " - 634s - loss: 0.6917 - accuracy: 0.5084 - val_loss: 0.6622 - val_accuracy: 0.6121\n",
      "Epoch 3/20\n",
      " - 658s - loss: 0.6260 - accuracy: 0.6571 - val_loss: 0.5868 - val_accuracy: 0.7044\n",
      "Epoch 4/20\n",
      " - 630s - loss: 0.5692 - accuracy: 0.7119 - val_loss: 0.5522 - val_accuracy: 0.7246\n",
      "Epoch 5/20\n",
      " - 638s - loss: 0.5435 - accuracy: 0.7326 - val_loss: 0.5383 - val_accuracy: 0.7379\n",
      "Epoch 6/20\n",
      " - 629s - loss: 0.5315 - accuracy: 0.7401 - val_loss: 0.5341 - val_accuracy: 0.7435\n",
      "Epoch 7/20\n",
      " - 641s - loss: 0.5113 - accuracy: 0.7538 - val_loss: 0.5112 - val_accuracy: 0.7534\n",
      "Epoch 8/20\n",
      " - 766s - loss: 0.4956 - accuracy: 0.7643 - val_loss: 0.5023 - val_accuracy: 0.7575\n",
      "Epoch 9/20\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "input_tensor = Input(shape=(214,))\n",
    "x = layers.Embedding(vocab_size, embedding_dim)(input_tensor)\n",
    "branch_a = layers.Conv1D(256, 5)(x)\n",
    "branch_a = layers.Dropout(0.5)(branch_a)\n",
    "branch_a = layers.Conv1D(128, 5)(branch_a)\n",
    "branch_a = layers.MaxPooling1D(pool_size=4)(branch_a)\n",
    "branch_a = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))(branch_a)\n",
    "branch_a = layers.LSTM(128, dropout=0.5, recurrent_dropout=0.5)(branch_a)\n",
    "branch_a = layers.Dense(128, activation='relu')(branch_a)\n",
    "\n",
    "branch_b = layers.Conv1D(256, 5)(x)\n",
    "branch_b = layers.MaxPooling1D(pool_size=4)(branch_b)\n",
    "branch_b = layers.LSTM(128, dropout=0.5, recurrent_dropout=0.5)(branch_b)\n",
    "branch_b = layers.Dense(128, activation='relu')(branch_b)\n",
    "\n",
    "merged = layers.concatenate([branch_a,branch_b], axis=-1)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "model = Model(input_tensor, predictions)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=np.array(training_sequences), y=np.array(training_labels), \n",
    "                    epochs=num_epochs, validation_data=(np.array(test_sequences),np.array(test_labels)), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxju4ItJKO8F"
   },
   "outputs": [],
   "source": [
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r')\n",
    "plt.plot(epochs, val_acc, 'b')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r')\n",
    "plt.plot(epochs, val_loss, 'b')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "# Expected Output\n",
    "# A chart where the validation loss does not increase sharply!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of NLP Course - Week 3 Exercise Question.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP%20Course%20-%20Week%203%20Exercise%20Question.ipynb",
     "timestamp": 1581533329242
    },
    {
     "file_id": "1bEdiniaTOd2jRxqGDoluQKXTRmYGJUy0",
     "timestamp": 1559930374235
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

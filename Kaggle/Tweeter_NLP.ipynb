{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmA6EzkQJ5jt"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "embedding_dim = 200\n",
    "max_length = 16\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size=160000\n",
    "test_portion= .1\n",
    "\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7135,
     "status": "ok",
     "timestamp": 1559930021999,
     "user": {
      "displayName": "Laurence Moroney",
      "photoUrl": "https://lh4.googleusercontent.com/-wUzpekukCVw/AAAAAAAAAAI/AAAAAAAAAHw/pQPstOOJqqE/s64/photo.jpg",
      "userId": "17858265307580721507"
     },
     "user_tz": 420
    },
    "id": "bM0l_dORKqE0",
    "outputId": "491ba86b-f780-4355-a4be-765565a29c8c"
   },
   "outputs": [],
   "source": [
    "# Note that I cleaned the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader\n",
    "# You can do that yourself with:\n",
    "# iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv\n",
    "# I then hosted it on my site to make it easier to use in this notebook\n",
    "\n",
    "#!wget --no-check-certificate \\\n",
    "#    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n",
    "#    -O /tmp/training_cleaned.csv\n",
    "\n",
    "# Your Code here. Create list items where the first item is the text, found in row[5], \n",
    "# and the second is the label. Note that the label is a '0' or a '4' in the text. When it's the former, make\n",
    "# your label to be 0, otherwise 1. Keep a count of the number of sentences in num_sentences\n",
    "\n",
    "num_sentences = 0\n",
    "\n",
    "with open(\"./datasets/sentiment140/training_cleaned.csv\", encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        list_item=[]\n",
    "        list_item.append(row[5])\n",
    "        this_label=row[0]\n",
    "        if this_label=='0':\n",
    "            list_item.append(0)\n",
    "        else:\n",
    "            list_item.append(1)\n",
    "        num_sentences = num_sentences + 1\n",
    "        corpus.append(list_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1559930024089,
     "user": {
      "displayName": "Laurence Moroney",
      "photoUrl": "https://lh4.googleusercontent.com/-wUzpekukCVw/AAAAAAAAAAI/AAAAAAAAAHw/pQPstOOJqqE/s64/photo.jpg",
      "userId": "17858265307580721507"
     },
     "user_tz": 420
    },
    "id": "3kxblBUjEUX-",
    "outputId": "3c0227a2-e74b-4d9b-cabb-f9ee150571b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "1600000\n",
      "[\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]\n"
     ]
    }
   ],
   "source": [
    "print(num_sentences)\n",
    "print(len(corpus))\n",
    "print(corpus[1])\n",
    "\n",
    "# Expected Output:\n",
    "# 1600000\n",
    "# 1600000\n",
    "# [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13089,
     "status": "ok",
     "timestamp": 1559930143373,
     "user": {
      "displayName": "Laurence Moroney",
      "photoUrl": "https://lh4.googleusercontent.com/-wUzpekukCVw/AAAAAAAAAAI/AAAAAAAAAHw/pQPstOOJqqE/s64/photo.jpg",
      "userId": "17858265307580721507"
     },
     "user_tz": 420
    },
    "id": "C1zdgJkusRh0",
    "outputId": "b6edd322-8191-45e7-cb12-08921685a72f"
   },
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sequence(text_sequence):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohOGz24lsNAD"
   },
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "labels=[]\n",
    "random.shuffle(corpus)\n",
    "for x in range(training_size):\n",
    "    sentences.append(corpus[x][0])\n",
    "    labels.append(corpus[x][1])\n",
    "\n",
    "sequences = [tokenize_sequence(sequence) for sequence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in sequences:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(test_portion * training_size)\n",
    "\n",
    "test_sequences = padded[0:split]\n",
    "training_sequences = padded[split:training_size]\n",
    "test_labels = labels[0:split]\n",
    "training_labels = labels[split:training_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKKvbuEBOGFz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = tf.keras.Sequential([\\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, trainable=False),\\n    tf.keras.layers.Conv1D(256, 5, activation=\\'relu\\'),\\n    tf.keras.layers.Dropout(0.5),\\n    tf.keras.layers.Conv1D(64, 5, activation=\\'relu\\'),\\n    tf.keras.layers.Dropout(0.5),\\n    tf.keras.layers.MaxPooling1D(pool_size=4),\\n    tf.keras.layers.LSTM(64),\\n    tf.keras.layers.Dense(1, activation=\\'sigmoid\\')\\n])\\n\\n\\nmodel.compile(loss=\\'binary_crossentropy\\',optimizer=\\'adam\\',metrics=[\\'accuracy\\'])\\nmodel.summary()\\n\\nnum_epochs = 50\\nhistory = model.fit(x=np.array(training_sequences), y=np.array(training_labels), epochs=num_epochs, validation_data=(np.array(test_sequences),np.array(test_labels)), batch_size = 1024, verbose=2)\\n\\nprint(\"Training Complete\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, trainable=False),\n",
    "    tf.keras.layers.Conv1D(256, 5, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "num_epochs = 50\n",
    "history = model.fit(x=np.array(training_sequences), y=np.array(training_labels), epochs=num_epochs, validation_data=(np.array(test_sequences),np.array(test_labels)), batch_size = 1024, verbose=2)\n",
    "\n",
    "print(\"Training Complete\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         6104400   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 256)         256256    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 64)          81984     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 256)         328704    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,968,593\n",
      "Trainable params: 6,968,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/20\n",
      "144000/144000 - 28s - loss: 0.6936 - accuracy: 0.4988 - val_loss: 0.6931 - val_accuracy: 0.5032\n",
      "Epoch 2/20\n",
      "144000/144000 - 23s - loss: 0.6507 - accuracy: 0.6031 - val_loss: 0.5227 - val_accuracy: 0.7544\n",
      "Epoch 3/20\n",
      "144000/144000 - 22s - loss: 0.4722 - accuracy: 0.7845 - val_loss: 0.4539 - val_accuracy: 0.7917\n",
      "Epoch 4/20\n",
      "144000/144000 - 23s - loss: 0.4184 - accuracy: 0.8119 - val_loss: 0.4557 - val_accuracy: 0.7930\n",
      "Epoch 5/20\n",
      "144000/144000 - 23s - loss: 0.3880 - accuracy: 0.8302 - val_loss: 0.4650 - val_accuracy: 0.7971\n",
      "Epoch 6/20\n",
      "144000/144000 - 23s - loss: 0.3537 - accuracy: 0.8481 - val_loss: 0.4738 - val_accuracy: 0.7955\n",
      "Epoch 7/20\n",
      "144000/144000 - 24s - loss: 0.3163 - accuracy: 0.8678 - val_loss: 0.4904 - val_accuracy: 0.7870\n",
      "Epoch 8/20\n",
      "144000/144000 - 24s - loss: 0.2881 - accuracy: 0.8819 - val_loss: 0.5610 - val_accuracy: 0.7822\n",
      "Epoch 9/20\n",
      "144000/144000 - 24s - loss: 0.2555 - accuracy: 0.8983 - val_loss: 0.5337 - val_accuracy: 0.7803\n",
      "Epoch 10/20\n",
      "144000/144000 - 23s - loss: 0.2304 - accuracy: 0.9080 - val_loss: 0.5991 - val_accuracy: 0.7814\n",
      "Epoch 11/20\n",
      "144000/144000 - 23s - loss: 0.2119 - accuracy: 0.9170 - val_loss: 0.6252 - val_accuracy: 0.7798\n",
      "Epoch 12/20\n",
      "144000/144000 - 23s - loss: 0.1923 - accuracy: 0.9251 - val_loss: 0.6909 - val_accuracy: 0.7722\n",
      "Epoch 13/20\n",
      "144000/144000 - 23s - loss: 0.1766 - accuracy: 0.9314 - val_loss: 0.7106 - val_accuracy: 0.7769\n",
      "Epoch 14/20\n",
      "144000/144000 - 23s - loss: 0.1621 - accuracy: 0.9379 - val_loss: 0.6781 - val_accuracy: 0.7711\n",
      "Epoch 15/20\n",
      "144000/144000 - 23s - loss: 0.1526 - accuracy: 0.9415 - val_loss: 0.7225 - val_accuracy: 0.7701\n",
      "Epoch 16/20\n",
      "144000/144000 - 22s - loss: 0.1409 - accuracy: 0.9458 - val_loss: 0.7893 - val_accuracy: 0.7740\n",
      "Epoch 17/20\n",
      "144000/144000 - 23s - loss: 0.1328 - accuracy: 0.9493 - val_loss: 0.7467 - val_accuracy: 0.7686\n",
      "Epoch 18/20\n",
      "144000/144000 - 22s - loss: 0.1275 - accuracy: 0.9516 - val_loss: 0.7503 - val_accuracy: 0.7758\n",
      "Epoch 19/20\n",
      "144000/144000 - 22s - loss: 0.1204 - accuracy: 0.9542 - val_loss: 0.8021 - val_accuracy: 0.7719\n",
      "Epoch 20/20\n",
      "144000/144000 - 22s - loss: 0.1127 - accuracy: 0.9572 - val_loss: 0.8387 - val_accuracy: 0.7752\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Conv1D(256, 5),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv1D(64, 5),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "num_epochs = 20\n",
    "history = model.fit(x=np.array(training_sequences), y=np.array(training_labels), epochs=num_epochs, validation_data=(np.array(test_sequences),np.array(test_labels)),batch_size = 2048, verbose=2)\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxju4ItJKO8F"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r')\n",
    "plt.plot(epochs, val_acc, 'b')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r')\n",
    "plt.plot(epochs, val_loss, 'b')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "# Expected Output\n",
    "# A chart where the validation loss does not increase sharply!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of NLP Course - Week 3 Exercise Question.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP%20Course%20-%20Week%203%20Exercise%20Question.ipynb",
     "timestamp": 1581533329242
    },
    {
     "file_id": "1bEdiniaTOd2jRxqGDoluQKXTRmYGJUy0",
     "timestamp": 1559930374235
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
